{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=16\n",
      "DROPOUT_KEEP_PROB=0.5\n",
      "EMBEDDING_DIM=128\n",
      "EVALUATE_EVERY=100\n",
      "FILTER_SIZES=3,4,5\n",
      "L2_REG_LAMBDA=0.0\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "NUM_EPOCHS=500\n",
      "NUM_FILTERS=128\n",
      "STOP_STEP=2500\n",
      "TASK=taskA\n",
      "TESTING_FILE=traindev/rumoureval-subtaskA-dev.json\n",
      "TRAINING_FILE=traindev/rumoureval-subtaskA-train.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#! /usr/bin/env python\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from text_cnn import TextCNN\n",
    "from tensorflow.contrib import learn\n",
    "import sys\n",
    "import operator\n",
    "import  json\n",
    "scriptpath = \"CNN_word embedding\"\n",
    "sys.path.append(os.path.abspath(scriptpath))\n",
    "from data_helpers import *\n",
    "\n",
    "# Parameters\n",
    "# ==================================================\n",
    "\n",
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularizaion lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 16, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 500, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"stop_step\", 2500, \"stop at (default: 1500)\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "# Task parameters\n",
    "tf.flags.DEFINE_string(\"task\", \"taskA\", \"semeval2017 TaskA\")\n",
    "tf.flags.DEFINE_string(\"training_file\", \"traindev/rumoureval-subtaskA-train.json\", \"file define training data set and annotation\")\n",
    "tf.flags.DEFINE_string(\"testing_file\", \"traindev/rumoureval-subtaskA-dev.json\", \"file define training data set and annotation (not use)\")\n",
    "\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")\n",
    "\n",
    "task=FLAGS.task\n",
    "training_file=FLAGS.training_file\n",
    "testing_file=FLAGS.testing_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparatopn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "global testID\n",
    "x_train, y_train,trainID = data_helpers.load_data_and_labels(task,training_file)\n",
    "x_dev,y_dev,testID= data_helpers.load_data_and_labels(task,testing_file)\n",
    "  \n",
    "# Build vocabulary\n",
    "\n",
    "x_text=x_train+x_dev\n",
    "max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
    "\n",
    "#Split train/test set\n",
    "x_train=x[:len(x_train)]\n",
    "x_dev=x[len(x_train):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training...\n",
      "eval2017-02-06T19:55:48.772336: step 100, loss 1.24274, acc 0.501779\n",
      "eval2017-02-06T19:55:53.305783: step 200, loss 0.951441, acc 0.604982\n",
      "eval2017-02-06T19:55:59.921325: step 300, loss 1.21974, acc 0.519573\n",
      "eval2017-02-06T19:56:05.702746: step 400, loss 1.09455, acc 0.548043\n",
      "eval2017-02-06T19:56:11.787609: step 500, loss 0.95713, acc 0.608541\n",
      "eval2017-02-06T19:56:18.484471: step 600, loss 0.944005, acc 0.622776\n",
      "eval2017-02-06T19:56:24.977552: step 700, loss 0.968025, acc 0.587189\n",
      "eval2017-02-06T19:56:30.575334: step 800, loss 0.905749, acc 0.622776\n",
      "eval2017-02-06T19:56:35.524309: step 900, loss 0.90247, acc 0.647687\n",
      "eval2017-02-06T19:56:42.202530: step 1000, loss 0.927396, acc 0.640569\n",
      "eval2017-02-06T19:56:49.651422: step 1100, loss 0.892626, acc 0.633452\n",
      "eval2017-02-06T19:56:55.285558: step 1200, loss 0.899466, acc 0.644128\n",
      "eval2017-02-06T19:57:01.523664: step 1300, loss 0.869524, acc 0.669039\n",
      "eval2017-02-06T19:57:06.418062: step 1400, loss 0.902667, acc 0.651246\n",
      "eval2017-02-06T19:57:11.908551: step 1500, loss 0.898459, acc 0.661922\n",
      "eval2017-02-06T19:57:18.493995: step 1600, loss 0.900401, acc 0.651246\n",
      "eval2017-02-06T19:57:24.720799: step 1700, loss 0.890921, acc 0.651246\n",
      "eval2017-02-06T19:57:31.172116: step 1800, loss 0.925289, acc 0.604982\n",
      "eval2017-02-06T19:57:37.746221: step 1900, loss 0.913701, acc 0.654804\n",
      "eval2017-02-06T19:57:43.026923: step 2000, loss 0.966924, acc 0.651246\n",
      "eval2017-02-06T19:57:48.213329: step 2100, loss 0.942851, acc 0.629893\n",
      "eval2017-02-06T19:57:54.726685: step 2200, loss 1.03, acc 0.640569\n",
      "eval2017-02-06T19:58:01.125662: step 2300, loss 0.939059, acc 0.637011\n",
      "eval2017-02-06T19:58:07.250184: step 2400, loss 1.08607, acc 0.651246\n",
      "eval2017-02-06T19:58:13.346364: step 2500, loss 1.03547, acc 0.597865\n"
     ]
    }
   ],
   "source": [
    "print(\"start training...\")\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=4,\n",
    "            vocab_size=len(vocab_processor.vocabulary_),\n",
    "            embedding_size=FLAGS.embedding_dim,\n",
    "            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "            num_filters=FLAGS.num_filters,\n",
    "            l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        \n",
    "        # Initialize all variables\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "            }\n",
    "            _, step,  loss, accuracy = sess.run(\n",
    "                [train_op, global_step,  cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            #print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "\n",
    "        def write_json(scores):\n",
    "            f=open(\"result.json\",\"w\")\n",
    "            output_dict={}\n",
    "            for i in xrange(len(testID)):\n",
    "                m=max(scores[i])\n",
    "                position=[a for a, j in enumerate(scores[i]) if j == m]\n",
    "                if type(position)==list:\n",
    "                    position=position[0]\n",
    "\n",
    "                if (position==0):\n",
    "                    category=\"comment\"\n",
    "                if (position==1):\n",
    "                    category=\"query\"\n",
    "                if (position==2):\n",
    "                    category=\"deny\"\n",
    "                if (position==3):\n",
    "                    category=\"support\"\n",
    "                output_dict[str(testID[i])]=category\n",
    "            f.write(json.dumps(output_dict,separators=(',', ': ')))\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "          \n",
    "            fo=open(\"result.txt\",\"w\")\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step,  loss, accuracy,scores = sess.run(\n",
    "                [global_step,  cnn.loss, cnn.accuracy,cnn.scores],\n",
    "                feed_dict)\n",
    "            y_label=[]\n",
    "            for i in y_batch:\n",
    "                for x in xrange(4):\n",
    "                    if(i[x]==1):\n",
    "                         y_label.append(x)\n",
    "\n",
    "            for ind in xrange(len(scores)):\n",
    "                fo.write (str(scores[ind])+\",\"+str(y_label[ind])+\"\\n\")\n",
    "            fo.write(\"\\n\")\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"eval{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            write_json(scores)\n",
    "\n",
    "\n",
    "        # Generate batches\n",
    "        batches = data_helpers.batch_iter(\n",
    "            list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "\t#print batch\n",
    "            x_batch, y_batch = zip(*batch)  \n",
    "\t#print x_batch\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "\n",
    "            if current_step % FLAGS.evaluate_every == 0:\n",
    "                #print(\"\\nEvaluation:\")\n",
    "                dev_step(x_dev, y_dev)\n",
    "            if(current_step==FLAGS.stop_step):\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
