{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Parameters setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=64\n",
      "CHECKPOINT_EVERY=400\n",
      "DROPOUT_KEEP_PROB=0.5\n",
      "EMBEDDING_DIM=128\n",
      "EVALUATE_EVERY=400\n",
      "EVENT=sydneysiege\n",
      "FILTER_SIZES=3,4,5\n",
      "L2_REG_LAMBDA=0.0\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "NUM_EPOCHS=40\n",
      "NUM_FILTERS=128\n",
      "STOP_STEP=400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn, rnn_cell\n",
    "import numpy as np\n",
    "import data_helpers\n",
    "from tensorflow.contrib import learn\n",
    "import os\n",
    "\n",
    "\n",
    "# Parameters\n",
    "# ==================================================\n",
    "\n",
    "\n",
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularizaion lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 40, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 400, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 400, \"Save model after this many steps (default: 100)\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "tf.flags.DEFINE_boolean(\"stop_step\", 400, \"model training times\")\n",
    "tf.flags.DEFINE_string(\"event\", \"sydneysiege\", \"event name (default: sydneysiege)\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")\n",
    "\n",
    "event=FLAGS.event\n",
    "fo=open(event+\"_rst.txt\",\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Preparatopn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "x_text, y_label = data_helpers.load_data_and_labels()\n",
    "\n",
    "#class_list=[\"ottawashooting\",\"charliehebdo\",\"ferguson\",\"sydneysiege\"]\n",
    "\n",
    "my_list=[]\n",
    "\n",
    "for x in xrange(len(x_text)):\n",
    "    i=x_text[x].find('*')\n",
    "    sub=x_text[x][:i]\n",
    "    x_text[x]=x_text[x][i+1:]\n",
    "    if(sub==event):\n",
    "        i1=x_text[x].find('*')\n",
    "        sub1=x_text[x][:i1]#threadID      \n",
    "        #x_text[x]=x_text[x][i1:]      \n",
    "        my_list.append(sub1)\n",
    "\n",
    "my_list = list(set(my_list)) #threadID list  \n",
    "test_num=[]\n",
    "for rum in my_list:            \n",
    "    temp=[]    \n",
    "    for x in xrange(len(x_text)):\n",
    "        i1=x_text[x].find('*')\n",
    "        if (i1!=-1):\n",
    "            if(rum==x_text[x][:i1]):\n",
    "                temp.append(x)\n",
    "                #x_text[x]=x_text[x][i1:]\n",
    "    test_num.append(temp)\n",
    "\n",
    "#data sub string\n",
    "for x in xrange(len(x_text)):\n",
    "    x_text[x]=x_text[x][19:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#build word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outfile=\"vocab\"\n",
    "max_document_length = max([len(x.split(\" \")) for x in x_text]) # Get vocabulary length\n",
    "if os.path.isfile(outfile+\".npy\"):\n",
    "    x_=np.load(outfile+\".npy\")\n",
    "else:\n",
    "   \n",
    "    #Return word2vec\n",
    "    sub_vec=data_helpers.word_vec_setup()\n",
    "    x_vec=[]\n",
    "    for k in x_text:\n",
    "        k_sp=k.split(\" \")#each word in instance\n",
    "        sent_vec=[]\n",
    "        i=-1\n",
    "        for _ in xrange(len(k_sp)):\n",
    "            i+=1    \n",
    "            sent_vec=np.append(sent_vec,data_helpers.word_vec_lookup(k_sp[i],sub_vec))\n",
    "        for _ in xrange(i+1,max_document_length):\n",
    "            sent_vec=np.append(sent_vec,np.array([0]*25))        \n",
    "        x_vec=np.append(x_vec,sent_vec)\n",
    "    x_vec.shape=(-1,max_document_length,25)\n",
    "    np.save(outfile, x_vec)#save vec to file\n",
    "    x_=x_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 10\n",
    "batch_size = FLAGS.batch_size\n",
    "display_step = 100\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 25 # MNIST data input (img shape: 28*28)\n",
    "n_steps = max_document_length # timesteps\n",
    "n_hidden = 128 # hidden layer num of features\n",
    "n_classes = 4 # MNIST total classes (0-9 digits)\n",
    "\n",
    "model_n=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Dev split: 4116/33\n",
      "Iter 6400, Minibatch Loss= 1.231309, Training Accuracy= 0.35938\n",
      "Iter 12800, Minibatch Loss= 1.109511, Training Accuracy= 0.48438\n",
      "Iter 19200, Minibatch Loss= 0.970205, Training Accuracy= 0.50000\n",
      "Iter 25600, Minibatch Loss= 0.964686, Training Accuracy= 0.51562\n",
      "Iter 32000, Minibatch Loss= 1.015546, Training Accuracy= 0.59375\n",
      "Iter 38400, Minibatch Loss= 0.793026, Training Accuracy= 0.67188\n",
      "Iter 44800, Minibatch Loss= 0.986932, Training Accuracy= 0.54688\n",
      "Iter 51200, Minibatch Loss= 0.840215, Training Accuracy= 0.60938\n",
      "Iter 57600, Minibatch Loss= 0.787711, Training Accuracy= 0.73438\n",
      "Iter 64000, Minibatch Loss= 0.697850, Training Accuracy= 0.68750\n",
      "Iter 70400, Minibatch Loss= 0.542169, Training Accuracy= 0.84375\n",
      "Iter 76800, Minibatch Loss= 0.533205, Training Accuracy= 0.76562\n",
      "Iter 83200, Minibatch Loss= 0.420205, Training Accuracy= 0.79688\n",
      "Iter 89600, Minibatch Loss= 0.446040, Training Accuracy= 0.82812\n",
      "Iter 96000, Minibatch Loss= 0.477115, Training Accuracy= 0.79688\n",
      "Iter 102400, Minibatch Loss= 0.518891, Training Accuracy= 0.76562\n",
      "Iter 108800, Minibatch Loss= 0.400065, Training Accuracy= 0.82812\n",
      "Iter 115200, Minibatch Loss= 0.321951, Training Accuracy= 0.87500\n",
      "Iter 121600, Minibatch Loss= 0.332548, Training Accuracy= 0.89062\n",
      "Iter 128000, Minibatch Loss= 0.480169, Training Accuracy= 0.79688\n",
      "Iter 134400, Minibatch Loss= 0.357144, Training Accuracy= 0.87500\n",
      "Iter 140800, Minibatch Loss= 0.329895, Training Accuracy= 0.90625\n",
      "Iter 147200, Minibatch Loss= 0.241769, Training Accuracy= 0.93750\n",
      "Iter 153600, Minibatch Loss= 0.289383, Training Accuracy= 0.85938\n",
      "Iter 160000, Minibatch Loss= 0.309557, Training Accuracy= 0.90625\n",
      "Iter 166400, Minibatch Loss= 0.222522, Training Accuracy= 0.89062\n",
      "Optimization Finished!\n",
      "0.606061\n",
      "Train/Dev split: 4137/12\n",
      "Iter 6400, Minibatch Loss= 1.289927, Training Accuracy= 0.42188\n",
      "Iter 12800, Minibatch Loss= 1.195776, Training Accuracy= 0.46875\n",
      "Iter 19200, Minibatch Loss= 1.203934, Training Accuracy= 0.51562\n",
      "Iter 25600, Minibatch Loss= 1.015693, Training Accuracy= 0.56250\n",
      "Iter 32000, Minibatch Loss= 1.126678, Training Accuracy= 0.56250\n",
      "Iter 38400, Minibatch Loss= 0.876006, Training Accuracy= 0.56250\n",
      "Iter 44800, Minibatch Loss= 1.012643, Training Accuracy= 0.57812\n",
      "Iter 51200, Minibatch Loss= 0.799312, Training Accuracy= 0.68750\n",
      "Iter 57600, Minibatch Loss= 0.855151, Training Accuracy= 0.65625\n",
      "Iter 64000, Minibatch Loss= 0.566662, Training Accuracy= 0.76562\n",
      "Iter 70400, Minibatch Loss= 0.721215, Training Accuracy= 0.64062\n",
      "Iter 76800, Minibatch Loss= 0.490559, Training Accuracy= 0.82812\n",
      "Iter 83200, Minibatch Loss= 0.440039, Training Accuracy= 0.81250\n",
      "Iter 89600, Minibatch Loss= 0.435518, Training Accuracy= 0.85938\n",
      "Iter 96000, Minibatch Loss= 0.446574, Training Accuracy= 0.82812\n",
      "Iter 102400, Minibatch Loss= 0.403135, Training Accuracy= 0.84375\n",
      "Iter 108800, Minibatch Loss= 0.292007, Training Accuracy= 0.87500\n",
      "Iter 115200, Minibatch Loss= 0.404207, Training Accuracy= 0.87500\n",
      "Iter 121600, Minibatch Loss= 0.368047, Training Accuracy= 0.82812\n",
      "Iter 128000, Minibatch Loss= 0.264027, Training Accuracy= 0.89062\n",
      "Iter 134400, Minibatch Loss= 0.259217, Training Accuracy= 0.89062\n",
      "Iter 140800, Minibatch Loss= 0.273854, Training Accuracy= 0.85938\n",
      "Iter 147200, Minibatch Loss= 0.164100, Training Accuracy= 0.95312\n",
      "Iter 153600, Minibatch Loss= 0.188385, Training Accuracy= 0.93750\n",
      "Iter 160000, Minibatch Loss= 0.165487, Training Accuracy= 0.95312\n",
      "Iter 166400, Minibatch Loss= 0.217379, Training Accuracy= 0.93750\n",
      "Optimization Finished!\n",
      "0.666667\n",
      "Train/Dev split: 4137/12\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-6701b858f06f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;31m# Define loss and optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# Evaluate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.pyc\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0maggregation_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation_method\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         grad_loss=grad_loss)\n\u001b[0m\u001b[1;32m    197\u001b[0m     return self.apply_gradients(grads_and_vars, global_step=global_step,\n\u001b[1;32m    198\u001b[0m                                 name=name)\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.pyc\u001b[0m in \u001b[0;36mcompute_gradients\u001b[0;34m(self, loss, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, grad_loss)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mgate_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgate_gradients\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGATE_OP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0maggregation_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation_method\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         colocate_gradients_with_ops=colocate_gradients_with_ops)\n\u001b[0m\u001b[1;32m    254\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgate_gradients\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGATE_GRAPH\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m       \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontrol_flow_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients.pyc\u001b[0m in \u001b[0;36mgradients\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method)\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0;31m# for ops that do not have gradients.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m               \u001b[0mgrad_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_gradient_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m               raise LookupError(\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mget_gradient_function\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1629\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m     \u001b[0mop_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_gradient_op_type\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1632\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m     \u001b[0mop_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mget_attr\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1518\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_node_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m       raise ValueError(\"No attr named '\" + name + \"' in \" +\n\u001b[0;32m-> 1520\u001b[0;31m                        str(self._node_def))\n\u001b[0m\u001b[1;32m   1521\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_node_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m     \u001b[0;31m# Treat an empty oneof value as an empty list.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/google/protobuf/internal/python_message.pyc\u001b[0m in \u001b[0;36m__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    981\u001b[0m   \u001b[0;34m\"\"\"Helper for _AddMessageMethods().\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 983\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtext_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMessageToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    984\u001b[0m   \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__str__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__str__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/google/protobuf/text_format.pyc\u001b[0m in \u001b[0;36mMessageToString\u001b[0;34m(message, as_utf8, as_one_line, pointy_brackets, use_index_order, float_format)\u001b[0m\n\u001b[1;32m    127\u001b[0m                \u001b[0mpointy_brackets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpointy_brackets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m                \u001b[0muse_index_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_index_order\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                float_format=float_format)\n\u001b[0m\u001b[1;32m    130\u001b[0m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m   \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/google/protobuf/text_format.pyc\u001b[0m in \u001b[0;36mPrintMessage\u001b[0;34m(message, out, indent, as_utf8, as_one_line, pointy_brackets, use_index_order, float_format)\u001b[0m\n\u001b[1;32m    170\u001b[0m                  \u001b[0mpointy_brackets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpointy_brackets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                  \u001b[0muse_index_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_index_order\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m                  float_format=float_format)\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/google/protobuf/text_format.pyc\u001b[0m in \u001b[0;36mPrintField\u001b[0;34m(field, value, out, indent, as_utf8, as_one_line, pointy_brackets, use_index_order, float_format)\u001b[0m\n\u001b[1;32m    179\u001b[0m   \"\"\"\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m   \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_extension\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'['\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/google/protobuf/text_format.pyc\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, val)\u001b[0m\n\u001b[1;32m     87\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPY2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for n in test_num:\n",
    "\n",
    "    model_n+=1\n",
    "    x_train,y_train=x_,y_label\n",
    "    x_dev,y_dev=[],[]\n",
    "    inplace=0\n",
    "\n",
    "    for num in n:\n",
    "        x_dev.append(x_[num])\n",
    "        y_dev.append(y_label[num])\n",
    "        x_train = np.delete(x_train, num-inplace,0)\n",
    "        y_train = np.delete(y_train, num-inplace,0) \n",
    "        inplace+=1\n",
    "        #print(x_text[num])\n",
    "        #print(x_[num])\n",
    "\n",
    "    #print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "    print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))\n",
    "\n",
    "\n",
    "        # tf Graph input\n",
    "    x = tf.placeholder(\"float\", [None, n_steps, n_input])\n",
    "    y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "    # Define weights\n",
    "    weights = {\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    def print_full(arr):\n",
    "        print (\"len(arr):\"+str(len(arr)))\n",
    "        print (\"len(arr[0]):\"+str(len(arr[0])))\n",
    "        aa=\"\"\n",
    "        for i in xrange(len(arr)):\n",
    "            for j in xrange(len(arr[0])):\n",
    "                aa+=str(arr[i][j])+\" \"\n",
    "            print (aa)\n",
    "            print (\"------------------------\")\n",
    "\n",
    "    def RNN(x, weights, biases,model_n):\n",
    "        # Prepare data shape to match `rnn` function requirements\n",
    "        # Current data input shape: (batch_size, n_steps, n_input)\n",
    "        # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "        # Permuting batch_size and n_steps\n",
    "        x = tf.transpose(x, [1, 0, 2])\n",
    "        # Reshaping to (n_steps*batch_size, n_input)\n",
    "        x = tf.reshape(x, [-1, n_input])\n",
    "        # Split to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "        x = tf.split(0, n_steps, x)\n",
    "\twith tf.variable_scope(\"myrnn\", reuse = None) as scope:\n",
    "            # Define a lstm cell with tensorflow\n",
    "\t    if (model_n>1):\n",
    "\t        scope.reuse_variables()#not to share variable\n",
    "            lstm_cell = rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0,state_is_tuple=True)\n",
    "            # Get lstm cell output\n",
    "            outputs, states = rnn.rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "            # Linear activation, using rnn inner loop last output\n",
    "        return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "    pred = RNN(x, weights, biases,model_n)\n",
    "\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "    # Evaluate model\n",
    "    correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    # Initializing the variables\n",
    "    init = tf.initialize_all_variables()\n",
    "\n",
    "    # Launch the graph\n",
    "    with tf.Session() as sess:\n",
    "\tfo=open(event+\"_rst.txt\",\"a\")\n",
    "        sess.run(init)\n",
    "        step = 1\n",
    "        # Keep training until reach max iterations\n",
    "        #while step * batch_size < training_iters:\n",
    "        batches = data_helpers.batch_iter(\n",
    "            list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)  \n",
    "\n",
    "\t    x_batch=np.array(x_batch)\n",
    "\t    y_batch=np.array(y_batch)\n",
    "           #print_full(batch_x[0])\n",
    "              # Run optimization op (backprop)\n",
    "            sess.run(optimizer, feed_dict={x: x_batch, y: y_batch})\n",
    "            if step % display_step == 0:\n",
    "                # Calculate batch accuracy\n",
    "                acc = sess.run(accuracy, feed_dict={x: x_batch, y: y_batch})\n",
    "                # Calculate batch loss\n",
    "                loss = sess.run(cost, feed_dict={x: x_batch, y: y_batch})\n",
    "                print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                      \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                      \"{:.5f}\".format(acc))\n",
    "            step += 1\n",
    "        print(\"Optimization Finished!\")\n",
    "\n",
    "        # Calculate accuracy \n",
    "\n",
    "        pred,accuracy=sess.run([pred,accuracy],feed_dict={x: x_dev, y: y_dev})\n",
    "        print(accuracy)\n",
    "        for i in xrange(len(pred)):\n",
    "\t    if (y_dev[i][0]==1):\n",
    "                fo.write (str(pred[i])+\", 0\\n\")\n",
    "\t    elif (y_dev[i][1]==1):\n",
    "                fo.write (str(pred[i])+\", 1\\n\")\n",
    "\t    elif (y_dev[i][2]==1):\n",
    "                fo.write (str(pred[i])+\", 2\\n\")\n",
    "\t    elif (y_dev[i][3]==1):\n",
    "                fo.write (str(pred[i])+\", 3\\n\")\n",
    "        fo.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
